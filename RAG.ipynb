{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "gn1SG3YGQhAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBmMVJWuvHCG",
        "outputId": "f917ce75-e56f-48c3-8fd5-cd7576d60132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.274)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: PyPDF2<3.0 in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20221105)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.27)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.2.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (41.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.6.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain transformers requests 'PyPDF2<3.0' pdfminer.six faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, BartForQuestionAnswering\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModelWithLMHead, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import text_splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "EReAJmlPvHpn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "   print(\"Training on GPU\")\n",
        "   device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ePg_SrB-MR",
        "outputId": "010c2218-5ace-43ed-8a75-4b8d54ccfa2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading text from file"
      ],
      "metadata": {
        "id": "pKaLIuXHQAYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "file_path = \"assets/attention.pdf\"\n",
        "\n",
        "if not os.path.exists('assets'):\n",
        "    os.mkdir('assets')\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    !curl -o $file_path $file_url\n",
        "else:\n",
        "    print(\"File already exists!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlACkxZZvP4p",
        "outputId": "493b24fe-c8c7-42e0-ee42-ce9a670691d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "zTQrmRLTfCry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Retriever:\n",
        "\n",
        "  def __init__(self, file_path, device, context_model_name, question_model_name):\n",
        "    self.file_path = file_path\n",
        "    self.device = device\n",
        "\n",
        "    self.context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(context_model_name)\n",
        "    self.context_model = DPRContextEncoder.from_pretrained(context_model_name).to(device)\n",
        "\n",
        "    self.question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(question_model_name)\n",
        "    self.question_model = DPRQuestionEncoder.from_pretrained(question_model_name).to(device)\n",
        "\n",
        "  def extract_text_from_pdf(self, file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "  def get_text(self):\n",
        "    with open(self.file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "  def token_len(self, text):\n",
        "    tokens = self.context_tokenizer.encode(text)\n",
        "    return len(tokens)\n",
        "\n",
        "  def load_chunks(self):\n",
        "    self.text = self.extract_text_from_pdf(self.file_path)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=50,\n",
        "        chunk_overlap=20,\n",
        "        length_function=self.token_len,\n",
        "        separators=[\"Section\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    self.chunks = text_splitter.split_text(self.text)\n",
        "\n",
        "  def load_context_embeddings(self):\n",
        "    encoded_input = self.context_tokenizer(self.chunks, return_tensors='pt', padding=True, truncation=True, max_length=300).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model_output = self.context_model(**encoded_input)\n",
        "      self.token_embeddings = model_output.pooler_output.cpu().detach().numpy()\n",
        "\n",
        "    self.index = faiss.IndexFlatL2(self.token_embeddings.shape[1])\n",
        "    self.index.add(self.token_embeddings)\n",
        "\n",
        "  def retrieve_top_k(self, query_prompt, k=10):\n",
        "    encoded_query = self.question_tokenizer(query_prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_output = self.question_model(**encoded_query)\n",
        "        query_vector = model_output.pooler_output\n",
        "\n",
        "    query_vector_np = query_vector.cpu().numpy()\n",
        "    D, I = self.index.search(query_vector_np, k)\n",
        "\n",
        "    retrieved_texts = [' '.join(self.chunks[i].split('\\n')) for i in I[0]]  # Replacing newlines with spaces\n",
        "\n",
        "    return retrieved_texts"
      ],
      "metadata": {
        "id": "kT-t3Q65Sfcn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAG:\n",
        "    def __init__(self,\n",
        "                 file_path,\n",
        "                 device,\n",
        "                 context_model_name=\"facebook/dpr-ctx_encoder-multiset-base\",\n",
        "                 question_model_name=\"facebook/dpr-question_encoder-multiset-base\",\n",
        "                 generator_name=\"valhalla/bart-large-finetuned-squadv1\"):\n",
        "\n",
        "      self.generator_name = generator_name\n",
        "      self.generator_tokenizer = AutoTokenizer.from_pretrained(generator_name)\n",
        "      self.generator_model = BartForQuestionAnswering.from_pretrained(generator_name).to(device)\n",
        "\n",
        "      self.retriever = Retriever(file_path, device, context_model_name, question_model_name)\n",
        "      self.retriever.load_chunks()\n",
        "      self.retriever.load_context_embeddings()\n",
        "\n",
        "    def abstractive_query(self, question):\n",
        "      self.generator_tokenizer = BartTokenizer.from_pretrained(self.generator_name)\n",
        "      self.generator_model = BartForConditionalGeneration.from_pretrained(self.generator_name).to(device)\n",
        "      context = self.retriever.retrieve_top_k(question, k=5)\n",
        "      # input_text = question + \" \" + \" \".join(context)\n",
        "      input_text = \"answer: \" + \" \".join(context) + \" \" + question\n",
        "\n",
        "      inputs = self.generator_tokenizer.encode(input_text, return_tensors='pt', max_length=500, truncation=True).to(device)\n",
        "      outputs = self.generator_model.generate(inputs, max_length=150, min_length=2, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "      answer = self.generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "      return answer\n",
        "\n",
        "    def extractive_query(self, question):\n",
        "      context = self.retriever.retrieve_top_k(question, k=15)\n",
        "      inputs = self.generator_tokenizer(question, \"context: \" + \". \".join(context), return_tensors=\"pt\", truncation=True, padding=True)\n",
        "      with torch.no_grad():\n",
        "        model_inputs = inputs.to(device)\n",
        "        outputs = self.generator_model(**model_inputs)\n",
        "\n",
        "      answer_start_index = outputs.start_logits.argmax()\n",
        "      answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "      if answer_end_index < answer_start_index:\n",
        "        answer_start_index, answer_end_index = answer_end_index, answer_start_index\n",
        "\n",
        "      predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "      answer = self.generator_tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
        "      answer = answer.replace('\\n', ' ').strip()\n",
        "      answer = answer.replace('$', '')\n",
        "\n",
        "      return answer"
      ],
      "metadata": {
        "id": "dMZQZNjIsbx7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "question_model_name = \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "# context_model_name=\"facebook/dpr-ctx_encoder-multiset-base\"\n",
        "# question_model_name=\"facebook/dpr-question_encoder-multiset-base\""
      ],
      "metadata": {
        "id": "p_V6j49TXIye"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgmIPuSiUoo_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Retriever(file_path, device, context_model_name, question_model_name)\n",
        "retriever.load_chunks()\n",
        "retriever.load_context_embeddings()"
      ],
      "metadata": {
        "id": "t5Uv8LXPWxsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fc5c83-7c09-4c01-acab-96a16734a8b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6093 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"What is the self-attention mechanism also known as?\",\n",
        "    \"What is another name for self-attention?\",\n",
        "    \"What are the benefits of using multiple attention heads?\",\n",
        "    \"Why do we use multiple attention heads?\",\n",
        "    \"What is the benefit of multi-head attention?\",\n",
        "    \"Can you explain the Transformer architecture to me in simple terms?\",\n",
        "    \"How is the self-attention mechanism different from other attention mechanisms?\",\n",
        "    \"In what ways does self-attention improve model performance?\",\n",
        "    \"What's the purpose behind using self-attention in the Transformer?\",\n",
        "    \"What problem does multi-head attention solve in the Transformer architecture?\",\n",
        "    \"How does the Transformer model use position encodings?\",\n",
        "    \"What are the main components of the Transformer architecture?\",\n",
        "    \"Describe the role of key-value pairs in the attention mechanism.\",\n",
        "    \"How does attention mechanism handle sequence order?\",\n",
        "    \"Why are positional encodings crucial in Transformers?\",\n",
        "    \"Can you outline the advantages of the Transformer model over RNNs?\",\n",
        "]\n",
        "query = queries[0]"
      ],
      "metadata": {
        "id": "4IJPjQvYZzTX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, context in enumerate(retriever.retrieve_top_k(query)):\n",
        "  print(f'{i+1}: \\t {context}\\n')\n",
        ""
      ],
      "metadata": {
        "id": "mY70TV32X_qA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae84aa8d-bca8-495f-f6e0-69a29a2ecce9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: \t described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
            "\n",
            "2: \t attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
            "\n",
            "3: \t Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
            "\n",
            "4: \t 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "\n",
            "5: \t during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "\n",
            "6: \t attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "\n",
            "7: \t because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention\n",
            "\n",
            "8: \t mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "\n",
            "9: \t Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n)\n",
            "\n",
            "10: \t self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag = RAG(file_path, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDXEBm8DX2C4",
        "outputId": "50804f51-c26f-46d5-9a3c-49471f25d1d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6093 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for query in queries:\n",
        "  print(f'Question: {query} \\n Answer: {rag.extractive_query(query)}')\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "1CFNdkIOX4BF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2134df27-1d79-4f6c-efaa-d51c353b5b79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the self-attention mechanism also known as? \n",
            " Answer: intra-attention is an attention mechanism relating different positions. attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.. Self-attention, sometimes called intra-attention\n",
            "\n",
            "\n",
            "Question: What is another name for self-attention? \n",
            " Answer: intra-attention\n",
            "\n",
            "\n",
            "Question: What are the benefits of using multiple attention heads? \n",
            " Answer: reduced effective resolution\n",
            "\n",
            "\n",
            "Question: Why do we use multiple attention heads? \n",
            " Answer: to efficiently handle large inputs and outputs\n",
            "\n",
            "\n",
            "Question: What is the benefit of multi-head attention? \n",
            " Answer: significantly faster\n",
            "\n",
            "\n",
            "Question: Can you explain the Transformer architecture to me in simple terms? \n",
            " Answer: stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,. To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the. mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "\n",
            "\n",
            "Question: How is the self-attention mechanism different from other attention mechanisms? \n",
            " Answer: relating different positions\n",
            "\n",
            "\n",
            "Question: In what ways does self-attention improve model performance? \n",
            " Answer: more interpretable models\n",
            "\n",
            "\n",
            "Question: What's the purpose behind using self-attention in the Transformer? \n",
            " Answer: compute representations of its input and output\n",
            "\n",
            "\n",
            "Question: What problem does multi-head attention solve in the Transformer architecture? \n",
            " Answer: draw global dependencies between input and output\n",
            "\n",
            "\n",
            "Question: How does the Transformer model use position encodings? \n",
            " Answer: stacked self-attention and point-wise, fully connected layers\n",
            "\n",
            "\n",
            "Question: What are the main components of the Transformer architecture? \n",
            " Answer: stacked self-attention and point-wise, fully connected layers for both the encoder and decoder\n",
            "\n",
            "\n",
            "Question: Describe the role of key-value pairs in the attention mechanism. \n",
            " Answer: Desc\n",
            "\n",
            "\n",
            "Question: How does attention mechanism handle sequence order? \n",
            " Answer: we must inject some information\n",
            "\n",
            "\n",
            "Question: Why are positional encodings crucial in Transformers? \n",
            " Answer: property. We implement this. efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].. and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the. encoder. •Similarly, self\n",
            "\n",
            "\n",
            "Question: Can you outline the advantages of the Transformer model over RNNs? \n",
            " Answer: mechanisms, dispensing with recurrence and convolutions. architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output. Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [ 37], the\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on text splitter:\n",
        "- Prioritizing periods (\".\") over whitespace (\" \") led to more cohesive English phrases but also led to more chunks with only decimal numbers\n",
        "- Including newlines (\"\\n\") led to undesirable behavior for chunking figures/diagrams as many of the components are separated by a newline"
      ],
      "metadata": {
        "id": "vf7kSV5LHd_L"
      }
    }
  ]
}